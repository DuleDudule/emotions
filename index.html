<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />

    <title>emotions-webapp</title>

    
    <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
    
    <script defer src="https://pyscript.net/latest/pyscript.js"></script>
    <py-config>
      packages = [ "matplotlib", "numpy","python-dotenv"] 
      [[fetch]]
      files = ["/request.py"]
      
    </py-config>

  </head>
  <style>
    #div1{
    display: flex;
    justify-content: center;
    
  }
    #div2{
    display: flex;
    justify-content: center;
    
  }
    #images{
      display: flex;
      justify-content: center;
  }
    #graph-area{
      display: flex;
      justify-content: center;
  }
    #dugmici{
      display: flex;
      justify-content: center;
  }
    #image-area{
      display: flex;
      justify-content: center;
      width:400px;
      height:400px;
      padding:5%;
  }
  #title{
      display: flex;
      justify-content: center;
      font-family: 'Courier New', monospace;
  }

  .myButton {
    background-color:#44c767;
    border-radius:28px;
    border:1px solid #18ab29;
    display:inline-block;
    cursor:pointer;
    color:#ffffff;
    font-family:Arial;
    font-size:15px;
    padding:8px 25px;
    text-decoration:none;
    text-shadow:0px 1px 0px #2f6627;
  }
  .myButton:hover {
    background-color:#5cbf2a;
  }
  #recordingProgress{
    display: flex;
    justify-content: center;
    color:#18ab29;
  }
  </style>
  <body>
    <h2 id="title">Write a prompt or record it as audio</h2>
    <div id="div2">
      <input type="text" id = "input_field" size = "50">
    </div>

    <h3 id = "recordingProgress"></h3>

    <div id ="dugmici">
      <button  onclick="startRecording()" class ="myButton">Start Recording</button>
      <button  onclick="stopRecording()" class ="myButton">Stop Recording</button>
      <button  onclick="playRecording()" class ="myButton">Play Recording</button>
      <button  onclick="postJSON()" class ="myButton">Transcribe audio</button>
    </div>




    <script>
      document.getElementById("input_field").value = "";
    </script>



    
    <br>

    <div id="div1">
      <button py-click="display_to_div()" class ="myButton" type = "submit">Detect emotions</button>
      <button py-click="generatePrompt()" class ="myButton" type = "submit">Generate prompt</button>
      <button py-click="generateImage()" class ="myButton" type = "submit">Submit prompt</button>
      <button py-click="generateImageEmotion()" class ="myButton" type = "submit">Submit emotion</button>
      
    </div>

    <py-script>
        
      import asyncio
      import js
      import json
      import io
      import os
      from dotenv import dotenv_values
      from PIL import Image
      import matplotlib.pyplot as plt
      from request import request  # import our request function.

      #load_dotenv()
      emotions = dict([])

      async def display_to_div():
        def plot(data):
          plt.rcParams["figure.figsize"] = (6,5)
          fig, ax = plt.subplots()
          keys = list(data.keys())
          values = list(data.values())
          key_colors = {'joy':'yellow','sadness':'grey','anger':'black','surprise':'blue','fear':'red','neutral':'orange','disgust':'green'}
          colors = [key_colors[xval] for xval in keys]
          plt.bar(keys,values,color= colors)
          
          display(fig, target="graph-area", append=False)

        async def main():
          text = Element('input_field').element.value 
          API_TOKEN = "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"
          headers = {"Authorization": f"Bearer {API_TOKEN}"}
          API_URL = "https://api-inference.huggingface.co/models/j-hartmann/emotion-english-distilroberta-base"
  
          data = json.dumps({"inputs": text})

          # POST
          new_post = await request(API_URL, body=data, method="POST", headers=headers)
          str = json.dumps(await new_post.json())
          return str
            

        
        text = Element('input_field').element.value
        

        if not text:
          Element('recordingProgress').element.innerText = "Write a prompt or record audio first"
        else:
          res = await asyncio.ensure_future(main())
          new_res = res[2:-2]
          res = new_res[1:-1]
          
          res_list = res.split(r'}, {')
          
          for el in res_list:
            el_list = el.split(",")

            first_split = el_list[0].split(':')
            label = first_split[1][2:-1]
            second_split = el_list[1].split(':')
            score = float(second_split[1])

            emotions[label] = score
          
          plot(emotions)
  
        
      async def generateImage():



        async def sendRequest():
          text = Element('input_field').element.value 
          API_TOKEN = "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"
          headers = {"Authorization": f"Bearer {API_TOKEN}"}
          API_URL = "https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5"
          data = json.dumps({"inputs": text})
          new_post = await request(API_URL, body=data, method="POST", headers=headers)
          return new_post

        text = Element('input_field').element.value
        if not text:
          Element('recordingProgress').element.innerText = "Write a prompt or record audio first"
        else:
          res = await asyncio.ensure_future(sendRequest())
          image = Image.open(io.BytesIO(await res.bytes()))
          display(image, target="image-area", append=False)


      async def generateImageEmotion():

        async def sendRequestEmotion(emotion):
          
          API_TOKEN = "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"
          headers = {"Authorization": f"Bearer {API_TOKEN}"}
          API_URL = "https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5"
          #data = json.dumps({"inputs": f"abstract image protraying {emotion}"})
          data = json.dumps({"inputs": f"{emotion}"})
          new_post = await request(API_URL, body=data, method="POST", headers=headers)
          return new_post


        if not emotions:
          Element('recordingProgress').element.innerText = "Detect emotions in a prompt first"
        else:
          max_value = list(emotions.values())
          max_key= list(emotions.keys())
          emotion = max_key[max_value.index(max(max_value))]
          
          #print(emotion)
          res = await asyncio.ensure_future(sendRequestEmotion(emotion))
          image = Image.open(io.BytesIO(await res.bytes()))
          display(image, target="image-area", append=False)
      
      async def generatePrompt():
        if not emotions:
          Element('recordingProgress').element.innerText = "Detect emotions in a prompt first"
        else:
          
          API_URL = "https://api.openai.com/v1/chat/completions"
          API_KEY = "sk-xxx"  #ADD API KEY HERE
          max_value = list(emotions.values())
          max_key= list(emotions.keys())
          emotion = max_key[max_value.index(max(max_value))]

          headers = {"Authorization": f"Bearer {API_KEY}",
                     "content-type":"application/json"}
          data = json.dumps({
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": f"Write a detailed prompt for ai image generation software that asks it to generate an image portraying the following emotion : {emotion}. Output only the prompt"}]
          }
          )
          new_post = await request(API_URL, body=data, method="POST", headers=headers)
          str = json.dumps(await new_post.json())
          dict = json.loads(str)
          gpt_response = dict["choices"][0]["message"]["content"]
          
          Element('input_field').element.value = gpt_response[1:-1]
          
          
          
    </py-script>


    <script>
      let audioRecorded = false;
      var recorder;
      var audioChunks = [];
      var audioBlob;
      var audioUrl;
      var audio;
      var transcribed_text
      function startRecording() {
        if(audioRecorded == false){
          audioRecorded = true;
          document.getElementById("recordingProgress").innerText = "Recording in progress";
          document.getElementById("recordingProgress").style.color = "#18ab29";
          navigator.mediaDevices.getUserMedia({ audio: true })
            .then(function(stream) {
              recorder = new MediaRecorder(stream);
              recorder.start();
    
              recorder.addEventListener('dataavailable', function(event) {
                audioChunks.push(event.data);
              });
            })
            .catch(function(error) {
              console.log(error);
            });
        }else{
          document.getElementById("recordingProgress").innerText = "Audio already recorded, refresh the page";
          document.getElementById("recordingProgress").style.color = "#ff0000";
        }

      }
  
      function stopRecording() {
        if(audioRecorded){
          document.getElementById("recordingProgress").innerText = "Recording finished";
          document.getElementById("recordingProgress").style.color = "#ff0000";
          if (recorder) {
            recorder.stop();
          }   

          recorder.addEventListener('stop', function() {
            if (audioChunks.length > 0) {
              audioBlob = new Blob(audioChunks);
              audioUrl = URL.createObjectURL(audioBlob);
              audio = new Audio(audioUrl);
              document.body.appendChild(audio);
        
            }
          });
        }else{
          document.getElementById("recordingProgress").innerText = "Record audio first";
          document.getElementById("recordingProgress").style.color = "#ff0000";
        }
        
      }
  
      function playRecording() {
        if(audioRecorded){
          document.getElementById("recordingProgress").innerText = "";
          if (audioUrl) {
            audio = new Audio(audioUrl);
            audio.play();
			    }
        }else{
          document.getElementById("recordingProgress").innerText = "Record audio first";
          document.getElementById("recordingProgress").style.color = "#ff0000";
        }


		}

    async function postJSON() {
      if(audioRecorded){
        document.getElementById("recordingProgress").innerText = "Audio sent";
        document.getElementById("recordingProgress").style.color = "#18ab29";
        try {
        const response = await fetch("https://api-inference.huggingface.co/models/jonatasgrosman/wav2vec2-large-xlsr-53-english", {
          method: "POST", // or 'PUT'
          headers: { 'Content-Type': 'audio/webm',
                      'Authorization': 'Bearer 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' },
          //body: JSON.stringify({'inputs':audio}) // TODO
          body : audioBlob
        });

        const result = await response.json();
        console.log("Success:", result);
        transcribed_text_json = JSON.stringify(result);
        transcribed_text = transcribed_text_json.substring(9,transcribed_text_json.length - 2)
        document.getElementById("input_field").value = transcribed_text;
      } catch (error) {
        console.error("Error:", error);
      }
      }else{
        document.getElementById("recordingProgress").innerText = "Record audio first";
        document.getElementById("recordingProgress").style.color = "#ff0000";
      }

    }
    
    </script>
    <div id="images">
      <div id="graph-area"></div>
      <div id="image-area"></div>
    </div>
  </body>
</html>
