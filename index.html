<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />

    <title>emotions-webapp</title>

    
    <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
    
    <script defer src="https://pyscript.net/latest/pyscript.js"></script>
    <py-config>
      packages = [ "matplotlib", "numpy"] 
      [[fetch]]
      files = ["/request.py"]
      
    </py-config>

  </head>
  <style>
    #div1{
    display: flex;
    justify-content: center;
  }
    #graph-area{
      display: flex;
      justify-content: center;
  }
    #dugmici{
      display: flex;
      justify-content: center;
    }
  </style>
  <body>
    <div id="div1">
      <input type="text" id = "input_field" >
      <button py-click="display_to_div()" id ="submit_button" type = "submit">Submit</button>
      
    </div>

    <script>
      document.getElementById("input_field").value = " ";
    </script>
    <div id ="output"></div>
    <py-script>
        
        import asyncio
        import js
        import json
        
        import matplotlib.pyplot as plt
        from request import request  # import our request function.

        
        
        async def display_to_div():
            async def main():
                
                #async def display_to_div():
                text = Element('input_field').element.value 
                API_TOKEN = "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"
                headers = {"Authorization": f"Bearer {API_TOKEN}"}
                API_URL = "https://api-inference.huggingface.co/models/j-hartmann/emotion-english-distilroberta-base"
        
                data = json.dumps({"inputs": text})

                # POST
                new_post = await request(API_URL, body=data, method="POST", headers=headers)
                str = json.dumps(await new_post.json())
                return str
                

            res = await asyncio.ensure_future(main())
            
            new_res = res[2:-2]
            res = new_res[1:-1]
            
            res_list = res.split(r'}, {')
            emotions = dict([])
            for el in res_list:
              el_list = el.split(",")

              first_split = el_list[0].split(':')
              label = first_split[1][2:-1]
              second_split = el_list[1].split(':')
              score = float(second_split[1])

              emotions[label] = score
              #print("-"+el)
            
            

            #Element('output').element.innerText = emotions
            
            def plot(data):
              plt.rcParams["figure.figsize"] = (6,5)
              fig, ax = plt.subplots()
              keys = list(data.keys())
              values = list(data.values())
              key_colors = {'joy':'yellow','sadness':'grey','anger':'black','surprise':'blue','fear':'red','neutral':'orange','disgust':'green'}
              colors = [key_colors[xval] for xval in keys]
              plt.bar(keys,values,color= colors)
              
              display(fig, target="graph-area", append=False)

            plot(emotions)
            
               
    </py-script>
    <br>
    <div id ="dugmici">
      <button  onclick="startRecording()">Start Recording</button>
      <button  onclick="stopRecording()">Stop Recording</button>
      <button  onclick="playRecording()">Play Recording</button>
      <button  onclick="postJSON()">Send to api</button>
    </div>

    <script>
      var recorder;
      var audioChunks = [];
      var audioBlob;
      var audioUrl;
      var audio;
      var transcribed_text
      function startRecording() {
        navigator.mediaDevices.getUserMedia({ audio: true })
          .then(function(stream) {
            recorder = new MediaRecorder(stream);
            recorder.start();
  
            recorder.addEventListener('dataavailable', function(event) {
              audioChunks.push(event.data);
            });
          })
          .catch(function(error) {
            console.log(error);
          });
      }
  
      function stopRecording() {
        if (recorder) {
          recorder.stop();
        }   

        recorder.addEventListener('stop', function() {
          if (audioChunks.length > 0) {
            audioBlob = new Blob(audioChunks);
            audioUrl = URL.createObjectURL(audioBlob);
            audio = new Audio(audioUrl);
            document.body.appendChild(audio);
      
          }
        });
      }
  
      function playRecording() {
			if (audioUrl) {
				audio = new Audio(audioUrl);
				audio.play();
			}


		}

    async function postJSON() {
      try {
        const response = await fetch("https://api-inference.huggingface.co/models/jonatasgrosman/wav2vec2-large-xlsr-53-english", {
          method: "POST", // or 'PUT'
          headers: { 'Content-Type': 'audio/webm',
                      'Authorization': 'Bearer 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' },
          //body: JSON.stringify({'inputs':audio}) // TODO
          body : audioBlob
        });

        const result = await response.json();
        console.log("Success:", result);
        transcribed_text_json = JSON.stringify(result);
        transcribed_text = transcribed_text_json.substring(9,transcribed_text_json.length - 2)
        document.getElementById("input_field").value = transcribed_text;
      } catch (error) {
        console.error("Error:", error);
      }
    }
    
    </script>
    <py-script>

    </py-script>
    <div id="graph-area"></div>
  </body>
</html>
